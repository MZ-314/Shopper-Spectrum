{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Shopper Spectrum\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Machine Learning - Clustering\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Mustafiz Ahmed"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shopper Spectrum project aims to leverage customer transactional data to perform intelligent segmentation and provide personalized product recommendations within an e-commerce context. Using the online_retail.csv dataset, the project implements RFM (Recency, Frequency, Monetary) analysis to classify customers based on their shopping behaviors and value to the business. This segmentation enables the identification of high-value, at-risk, and potential customers, helping the business adopt tailored marketing strategies.\n",
        "\n",
        "The project also builds a collaborative filtering–based recommendation system that uses customer-item purchase patterns to recommend relevant products. By combining segmentation with personalized suggestions, the project addresses both customer retention and revenue generation goals.\n",
        "\n",
        "Furthermore, an interactive Streamlit web application is developed to provide business users with an easy-to-navigate dashboard for exploring segments and generating live recommendations. The platform combines clustering insights and real-time suggestions, transforming static insights into actionable business strategies.\n",
        "\n",
        "Overall, Shopper Spectrum demonstrates the end-to-end process of customer analytics — from raw data preprocessing and feature engineering to unsupervised learning and deployment. It delivers scalable, data-driven solutions to improve customer engagement, increase retention, and maximize lifetime value."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/MZ-314/Shopper-Spectrum"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E-commerce platforms serve a wide range of customers, each with distinct buying behaviors and preferences. However, treating all customers the same leads to missed marketing opportunities and reduced retention. Without customer segmentation and personalized recommendations, businesses struggle to engage users meaningfully, predict future behavior, or prioritize valuable customers.\n",
        "\n",
        "The objective of this project is twofold:\n",
        "\n",
        "Segment customers based on behavioral data using RFM analysis and clustering to identify loyal, inactive, or high-potential customers.\n",
        "\n",
        "Recommend products to customers using item-based collaborative filtering based on past purchase patterns.\n",
        "\n",
        "By solving this, the business can target customers more strategically, offer better deals, reduce churn, and ultimately boost customer satisfaction and revenue."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "JzPtS-j9kk8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation and numerical operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Date and time handling\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning - Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Distance calculation\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# For collaborative filtering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Streamlit app (for deployment)\n",
        "import streamlit as st\n",
        "\n",
        "# Evaluation and model utilities (if needed later)\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('online_retail.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows and columns\n",
        "rows, columns = df.shape\n",
        "print(f\"The dataset contains {rows} rows and {columns} columns.\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset information\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of duplicate rows in the dataset\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows in the dataset: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Display only columns with at least one missing value\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "print(\"Missing values in each column:\\n\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='Reds')\n",
        "plt.title(\"Heatmap of Missing Values\", fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains over 540,000 transaction records from a UK-based online retail store between 2010 and 2011. Each row represents a single product purchase, including fields such as InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country. The dataset helps in understanding customer buying patterns, product popularity, and purchasing frequency. However, it also contains missing values (particularly in the CustomerID column) and some duplicate or cancelled transactions that need to be cleaned. This dataset is ideal for customer segmentation using RFM (Recency, Frequency, Monetary) analysis and for building a product recommendation system."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all column names\n",
        "print(\"Dataset Columns:\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for numeric columns\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains eight columns, each providing specific information about online retail transactions. The InvoiceNo column is a unique identifier for each transaction; if it begins with the letter \"C\", it denotes a cancelled order. The StockCode represents the unique code assigned to each product. Description provides the textual name or description of the purchased item. The Quantity column indicates the number of units purchased in the transaction, where negative values may represent product returns. InvoiceDate records the exact date and time of the transaction. The UnitPrice reflects the price per item in British Pounds (GBP). The CustomerID is a unique numeric identifier for each customer; this field contains some missing values for unidentified customers. Lastly, the Country column specifies the geographic location of the customer at the time of purchase."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of unique values per column\n",
        "unique_counts = df.nunique()\n",
        "print(\"Unique value count for each column:\\n\")\n",
        "print(unique_counts)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Drop rows with missing CustomerID\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# 2. Remove canceled orders (InvoiceNo starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# 3. Remove rows with negative or zero Quantity or UnitPrice\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "\n",
        "# 4. Convert InvoiceDate to datetime format\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# 5. Ensure CustomerID is treated as string (for RFM segmentation)\n",
        "df['CustomerID'] = df['CustomerID'].astype(str)\n",
        "\n",
        "# Reset index after filtering\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# View new shape\n",
        "print(\"Cleaned dataset shape:\", df.shape)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the dataset analysis-ready, several data wrangling steps were performed. First, all rows with missing CustomerID were removed since customer identification is critical for segmentation and personalized recommendations. Next, cancelled transactions were filtered out by excluding records where InvoiceNo started with the letter 'C'. Transactions with non-positive Quantity or UnitPrice were also removed as they indicate invalid or erroneous entries. The InvoiceDate column was converted to proper datetime format to enable time-based analysis, and CustomerID was cast to string to treat it as a categorical identifier. After cleaning, the dataset size was significantly reduced, improving data quality for meaningful insights. These manipulations helped eliminate noise and ensure accurate calculations in the upcoming RFM segmentation and recommendation tasks."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=top_countries.index, y=top_countries.values, palette=\"viridis\")\n",
        "plt.yscale('log')\n",
        "plt.title(\"Top 10 Countries by Number of Transactions (Log Scale)\")\n",
        "plt.ylabel(\"Transaction Count (log scale)\")\n",
        "plt.xlabel(\"Country\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was selected to visualize the distribution of quantities ordered across all transactions. This chart type helps in identifying the frequency of various quantity values and detecting any abnormalities or skewed behavior in ordering patterns."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that most orders are for low quantities—typically between 1 and 10 items. There is a sharp drop-off as quantities increase, with a few extreme outliers (e.g., 1000+ quantity), which could indicate bulk purchases or erroneous entries."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Understanding typical order sizes helps in optimizing logistics, inventory planning, and fulfillment efficiency. The presence of large outliers can also highlight potential data entry issues or bulk buyer segments worth special targeting. Addressing errors can improve data quality and customer satisfaction."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_products = df['Description'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=top_products.values, y=top_products.index, palette=\"magma\")\n",
        "plt.title(\"Top 10 Most Purchased Products\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Product\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram with KDE (Kernel Density Estimate) was chosen to explore the distribution of product pricing. This allows a smooth view of how products are priced and highlights the central tendencies and outliers."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most products are priced below £10, with a steep decline beyond that. A long right tail indicates a few high-priced items. This suggests a business model focused on low-cost, high-volume products with a few premium offerings."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. It supports pricing strategies tailored to the company’s main price segments. Identifying high-value products can inform promotions or bundling, while confirming that the bulk of the catalog targets budget-conscious consumers."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df[df['Quantity'] < 100]['Quantity'], bins=50, kde=True)\n",
        "plt.title(\"Distribution of Quantity Ordered (Filtered < 100)\")\n",
        "plt.xlabel(\"Quantity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was used to evaluate the spread of revenue per transaction, helping assess the business’s financial structure—whether most transactions are low- or high-value."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most transactions generate less than £100 in revenue, with a few exceptionally high revenue points. This indicates high reliance on smaller orders and potentially few large wholesale buyers."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This suggests marketing efforts should focus on increasing basket size for smaller orders, while also maintaining strong relationships with high-value clients. Fraud detection systems should review outlier transactions for anomalies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Filter out extreme outliers (e.g., prices > 50)\n",
        "filtered_df = df[df['UnitPrice'] < 50]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(filtered_df['UnitPrice'], bins=50, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of Unit Price (Filtered < £50)\")\n",
        "plt.xlabel(\"Price (GBP)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This refined histogram filters out high-price outliers to better visualize the common product price distribution. It improves readability and focuses analysis on the majority of the catalog."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering out prices above £50 reveals a tighter cluster of pricing around the £2–£10 range. This gives a more realistic picture of what regular customers typically spend."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This refined view confirms the business’s competitive pricing strategy and helps identify ideal price points for upselling and discount thresholds."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
        "monthly_sales = df.groupby('InvoiceMonth')['Quantity'].sum()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "monthly_sales.plot(marker='o')\n",
        "plt.title(\"Monthly Sales Trend\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Total Quantity Sold\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart is ideal for displaying categorical data with long labels (country names) and comparing order counts clearly."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excluding the UK, top order contributors include Germany, France, Netherlands, and others. This confirms demand from select international markets."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These insights allow the business to target international logistics improvements, create country-specific promotions, and focus retention efforts on these key non-UK markets."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate revenue (UnitPrice × Quantity)\n",
        "df['Revenue'] = df['UnitPrice'] * df['Quantity']\n",
        "\n",
        "# Group by country and sum revenue\n",
        "top_countries = df.groupby('Country')['Revenue'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=top_countries.index, y=top_countries.values, palette=\"coolwarm\")\n",
        "\n",
        "# Title and labels\n",
        "plt.title(\"Top 10 Countries by Revenue\", fontsize=14)\n",
        "plt.xlabel(\"Country\")\n",
        "plt.ylabel(\"Revenue (GBP)\")\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Annotate bars\n",
        "for i, v in enumerate(top_countries.values):\n",
        "    ax.text(i, v + (0.01 * v), f'£{int(v):,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart evaluates not just frequency of orders but the total revenue contribution per country, helping identify the most financially valuable markets."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some countries contribute more in revenue despite fewer orders, implying larger basket sizes or premium purchases."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Countries contributing higher revenue per transaction can be prioritized for loyalty programs, premium shipping services, or product expansions. Conversely, low-revenue countries may be reevaluated for profitability."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_customers = df.groupby('CustomerID')['Revenue'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=top_customers.index, y=top_customers.values, palette='Set2')\n",
        "plt.title(\"Top 10 Customers by Revenue\")\n",
        "plt.xlabel(\"Customer ID\")\n",
        "plt.ylabel(\"Revenue (GBP)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots help identify the top individual contributors to order volume, aiding customer-level analysis."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A handful of customers place significantly more orders than the rest, indicating VIP or business clients."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These customers are vital to business stability and should be enrolled in loyalty programs or offered exclusive benefits. Losing them could severely impact revenue, so retention is critical."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Hour'] = df['InvoiceDate'].dt.hour\n",
        "hourly_orders = df.groupby('Hour')['InvoiceNo'].nunique()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(x=hourly_orders.index, y=hourly_orders.values, marker='o')\n",
        "plt.title(\"Number of Orders by Hour\")\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Number of Orders\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While order volume is important, total revenue per customer shows who brings in the most income—even if they order less frequently."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some customers place fewer but high-value orders, pointing toward wholesale or premium buyers. This customer segment may not be as visible from order volume charts."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These insights inform segmentation strategies: offer value-based promotions, optimize shipping deals, and prioritize these users during service escalations."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_revenue = df.groupby('Description')['Revenue'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=product_revenue.values, y=product_revenue.index, palette=\"cubehelix\")\n",
        "plt.title(\"Top 10 Revenue-Generating Products\")\n",
        "plt.xlabel(\"Revenue (GBP)\")\n",
        "plt.ylabel(\"Product\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot over time identifies the specific days with peak sales, useful for temporal analysis and campaign performance tracking."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are noticeable sales peaks around specific dates—these may coincide with holidays, marketing events, or inventory clearances."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This supports planning for future promotions, staffing during peak demand, and seasonal inventory management. It also helps detect anomalies or fraud patterns on unusual high-sale days."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of orders per customer\n",
        "order_freq = df['CustomerID'].value_counts()\n",
        "\n",
        "# Plot histogram with log scale on x-axis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(order_freq, bins=50, kde=True, color='teal')\n",
        "plt.yscale('linear')  # optional: 'log' if needed\n",
        "plt.xlabel('Number of Orders')\n",
        "plt.ylabel('Customer Count')\n",
        "plt.title('Frequency of Orders by Customers')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram helps in understanding how often customers make purchases, identifying one-time vs. repeat buyers."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers are one-time or infrequent buyers. There is a small tail of highly engaged customers who buy regularly."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These insights enable segmentation for email marketing and remarketing efforts. One-time buyers could be nurtured into repeat customers with targeted campaigns."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 most frequently ordered products\n",
        "top_products = df['Description'].value_counts().head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_products.values, y=top_products.index, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Most Frequently Ordered Products\")\n",
        "plt.xlabel(\"Number of Orders\")\n",
        "plt.ylabel(\"Product Description\")\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart was ideal for visualizing the products with the highest order frequency while accommodating long product descriptions."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain products dominate the order list, indicating consistent customer interest or essential items."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These products can be promoted more aggressively, stocked adequately, or bundled with other items to boost sales of slower-moving stock."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a revenue column\n",
        "df['Revenue'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Group by product and sum revenue\n",
        "top_revenue_products = df.groupby('Description')['Revenue'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_revenue_products.values, y=top_revenue_products.index, palette=\"magma\")\n",
        "plt.title(\"Top 10 Products by Total Revenue\")\n",
        "plt.xlabel(\"Total Revenue (GBP)\")\n",
        "plt.ylabel(\"Product Description\")\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of volume, this chart shows top revenue-generating products, revealing financially most important SKUs."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few products contribute disproportionately to revenue, possibly due to high price points or frequent large orders."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Prioritizing marketing, stock, and supplier management around these products can significantly impact bottom-line growth."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert InvoiceDate to datetime if not already\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Create Year-Month column\n",
        "df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
        "\n",
        "# Group by InvoiceMonth and calculate monthly revenue\n",
        "monthly_revenue = df.groupby('InvoiceMonth')['Revenue'].sum().reset_index()\n",
        "monthly_revenue['InvoiceMonth'] = monthly_revenue['InvoiceMonth'].astype(str)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='InvoiceMonth', y='Revenue', data=monthly_revenue, marker='o', color='teal')\n",
        "plt.title(\"Monthly Revenue Trend\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Revenue (GBP)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot over time is best to evaluate performance trends, seasonality, and long-term growth."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revenue fluctuates by month, with visible spikes indicating high-performing periods. These might align with seasonal sales, promotions, or campaign rollouts."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This insight allows for future sales forecasting, campaign planning, and resource allocation to match expected seasonal trends."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns for correlation\n",
        "numeric_cols = df.select_dtypes(include='number')\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = numeric_cols.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap was chosen because it is one of the most effective visual tools to understand the linear relationships between multiple numerical features in a dataset. This chart provides a compact and intuitive representation of how strongly variables are related to each other, using color gradients to show the correlation coefficients. For a project that involves customer segmentation, product analysis, and revenue prediction, it's essential to detect multicollinearity, redundant features, or highly associated metrics (e.g., Quantity and Revenue). By visualizing these relationships, we can make informed decisions during feature selection, modeling, and transformation stages. This also helps identify features that might influence each other or the target variable the most."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap revealed several important correlations within the dataset. Notably, there was a strong positive correlation between Quantity and Revenue, indicating that higher quantities ordered naturally lead to higher transaction values. Another mild positive correlation was observed between UnitPrice and Revenue, but it was less significant than quantity. On the other hand, CustomerID and InvoiceNo showed almost no correlation with revenue, which makes sense as they are more categorical identifiers than numerical indicators. Additionally, there was minimal multicollinearity between other numerical variables, suggesting that each numerical feature carries relatively unique information, which is valuable for modeling and clustering tasks later in the pipeline."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select key numerical features for the pair plot\n",
        "pairplot_features = df[['Quantity', 'UnitPrice', 'Revenue']]\n",
        "\n",
        "# Plotting the pair plot\n",
        "sns.pairplot(pairplot_features, diag_kind='kde', corner=True, palette=\"Set2\")\n",
        "plt.suptitle(\"Pair Plot of Key Numerical Features\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot (also known as a scatterplot matrix) was chosen to visualize pairwise relationships between multiple numerical features in the dataset. This chart is particularly useful for detecting underlying patterns, trends, correlations, and clusters between features like Quantity, UnitPrice, and Revenue. It also displays the distribution of each variable along the diagonals using histograms or KDE plots, providing insights into individual feature distributions. In a customer segmentation and revenue analysis project, a pair plot helps assess how features interact and whether any natural groupings or anomalies are visually apparent, which is valuable before applying clustering or machine learning models."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot clearly showed a strong positive relationship between Quantity and Revenue, reaffirming the insight from the correlation heatmap. Most data points are concentrated in the lower-left quadrants of the scatterplots, indicating that both low quantity and low revenue transactions are the most common. Some scatterplots showed sparse data with a few extreme outliers, particularly in UnitPrice, which suggests the presence of high-value items or possibly pricing anomalies. The histograms along the diagonals confirmed that most features are right-skewed, especially Revenue, meaning a small number of transactions contribute a large portion of the overall revenue."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 1:\n",
        "Statement:\n",
        "“High-value customers (top 25% by total monetary value) generate more revenue per invoice than occasional customers (bottom 25%).”\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "There is no difference in the average revenue per invoice between high-value and occasional customers.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "High-value customers have a significantly higher average revenue per invoice than occasional customers.\n",
        "\n",
        "Hypothetical Statement 2:\n",
        "Statement:\n",
        "“Customers from the United Kingdom place higher quantity orders on average compared to customers from other countries.”\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "The mean quantity ordered by UK customers is equal to the mean quantity ordered by customers from other countries.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "The mean quantity ordered by UK customers is greater than the mean quantity ordered by customers from other countries.\n",
        "\n",
        "Hypothetical Statement 3:\n",
        "Statement:\n",
        "“Frequent buyers (top 25% in frequency) tend to buy at a lower average unit price compared to infrequent buyers (bottom 25%).”\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "There is no difference in the average unit price between frequent and infrequent buyers.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "Frequent buyers purchase at a lower average unit price than infrequent buyers."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 1:\n",
        "Statement:\n",
        "“High-value customers (top 25% by total monetary value) generate more revenue per invoice than occasional customers (bottom 25%).”\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "There is no difference in the average revenue per invoice between high-value and occasional customers.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "High-value customers have a significantly higher average revenue per invoice than occasional customers."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# 1. Compute RFM and define segments\n",
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'Revenue': 'sum'\n",
        "}).rename(columns={'InvoiceDate':'Recency',\n",
        "                   'InvoiceNo':'Frequency',\n",
        "                   'Revenue':'Monetary'})\n",
        "\n",
        "# Example thresholding: define high-value vs occasional\n",
        "high_value = rfm[rfm['Monetary'] >= rfm['Monetary'].quantile(0.75)]\n",
        "occasional = rfm[rfm['Monetary'] <= rfm['Monetary'].quantile(0.25)]\n",
        "\n",
        "# 2. Pull per-invoice revenue for each group\n",
        "#    (we need revenue per invoice: group original df)\n",
        "df['Revenue'] = df['Quantity'] * df['UnitPrice']\n",
        "rev_per_invoice = df.groupby(['CustomerID','InvoiceNo'])['Revenue'].sum().reset_index()\n",
        "\n",
        "high_rev = rev_per_invoice[rev_per_invoice['CustomerID'].isin(high_value.index)]['Revenue']\n",
        "occ_rev  = rev_per_invoice[rev_per_invoice['CustomerID'].isin(occasional.index)]['Revenue']\n",
        "\n",
        "# 3. Perform one‐tailed Welch’s t‐test\n",
        "t_stat, p_value = ttest_ind(high_rev, occ_rev, alternative='greater', equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", round(t_stat,3))\n",
        "print(\"P-value:\", round(p_value,4))\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothesis 1, I used the Welch’s t-test (a variation of the independent two-sample t-test) to compare the average revenue per invoice between high-value and occasional customers.\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose Welch’s t-test because the two customer groups being compared—high-value customers and occasional customers—are independent and likely to have different sample sizes and unequal variances in their revenue per invoice distributions. In such cases, the regular Student’s t-test is not appropriate, as it assumes equal variances (homoscedasticity), which doesn't hold true for real-world customer purchasing data.\n",
        "\n",
        "Welch’s t-test is a more reliable and robust alternative when these assumptions are violated. It adjusts the degrees of freedom and provides a more accurate p-value under unequal variance conditions, making it the best fit for this business scenario.\n",
        "\n",
        "This choice ensures that the statistical inference about customer value is valid, unbiased, and suitable for making data-driven business decisions."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 2:\n",
        "Statement:\n",
        "“Customers from the United Kingdom place higher quantity orders on average compared to customers from other countries.”\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "The mean quantity ordered by UK customers is equal to the mean quantity ordered by customers from other countries.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "The mean quantity ordered by UK customers is greater than the mean quantity ordered by customers from other countries."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Split into UK and Non-UK customers\n",
        "uk_customers = df[df['Country'] == 'United Kingdom']['Quantity']\n",
        "non_uk_customers = df[df['Country'] != 'United Kingdom']['Quantity']\n",
        "\n",
        "# Perform one-tailed Welch’s t-test\n",
        "t_stat2, p_value2 = ttest_ind(uk_customers, non_uk_customers, alternative='greater', equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", round(t_stat2, 3))\n",
        "print(\"P-value:\", round(p_value2, 4))\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an independent two-sample t-test (Welch’s t-test) again, comparing the quantity ordered by UK customers vs. customers from other countries."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two groups (UK vs. Non-UK customers) are independent and likely have unequal sample sizes and variances. Welch’s t-test is ideal in such cases as it does not assume equal variances. It’s robust and widely used for comparing means from two separate groups with real-world data variability."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 3:\n",
        "Statement:\n",
        "“Frequent buyers (top 25% in frequency) tend to buy at a lower average unit price compared to infrequent buyers (bottom 25%).”\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "There is no difference in the average unit price between frequent and infrequent buyers.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "Frequent buyers purchase at a lower average unit price than infrequent buyers."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Group by CustomerID and count number of invoices (frequency)\n",
        "frequency_df = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
        "frequency_df.columns = ['CustomerID', 'Frequency']\n",
        "\n",
        "# Merge with main data\n",
        "df = df.merge(frequency_df, on='CustomerID', how='left')\n",
        "\n",
        "# Define top 25% and bottom 25% based on frequency\n",
        "top_25 = df[df['Frequency'] >= df['Frequency'].quantile(0.75)]\n",
        "bottom_25 = df[df['Frequency'] <= df['Frequency'].quantile(0.25)]\n",
        "\n",
        "# Run Welch's t-test (one-tailed)\n",
        "t_stat3, p_value3 = ttest_ind(top_25['UnitPrice'], bottom_25['UnitPrice'], alternative='less', equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", round(t_stat3, 3))\n",
        "print(\"P-value:\", round(p_value3, 4))\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Welch’s t-test (one-tailed, unequal variance t-test) to compare the average unit prices of the top and bottom 25% customer segments by frequency."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, since the two groups (frequent vs. infrequent buyers) are independent, and their sample sizes and variances are unequal, Welch’s t-test is most appropriate. This test accounts for unequal variances and provides reliable results for determining if frequent buyers are more price-sensitive (paying less on average)."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n"
      ],
      "metadata": {
        "id": "EFVZYqjvRw8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where CustomerID is missing (essential for segmentation)\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# Fill missing 'Description' with 'Unknown'\n",
        "df['Description'] = df['Description'].fillna('Unknown')\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle missing values, we dropped rows with missing CustomerID since customer-level analysis requires proper identification. Missing values in the Description column were filled with 'Unknown' to retain the transaction record while marking the ambiguity in the product details."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing records with negative or zero quantities and unit prices\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We removed outliers where Quantity or UnitPrice was zero or negative, as such transactions are either cancellations or data entry errors and could mislead customer behavior or revenue analysis."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Country using Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Country_Code'] = le.fit_transform(df['Country'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Label Encoding on the Country variable to convert categorical country names into numerical values for further modeling or clustering. Label Encoding is sufficient here since there is no ordinal relationship among countries, and the country information will be used as a feature, not as a label."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "contractions_dict = {\"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\",\n",
        "                     \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"}\n",
        "\n",
        "def expand_contractions(text):\n",
        "    for contraction, expanded in contractions_dict.items():\n",
        "        text = re.sub(contraction, expanded, text)\n",
        "    return text\n",
        "\n",
        "df['Description'] = df['Description'].apply(lambda x: expand_contractions(str(x)))\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Description'] = df['Description'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Description'] = df['Description'].str.replace('[^\\w\\s]', '', regex=True)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Description'] = df['Description'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
        "df['Description'] = df['Description'].apply(lambda x: ' '.join([word for word in x.split() if not any(char.isdigit() for char in word)]))\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "dWfq74SkUKZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df['Description'] = df['Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "df['Description'] = df['Description'].apply(lambda x: re.sub('\\s+', ' ', x).strip())\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skipped as product names are short, structured strings; no need for rephrasing."
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "Y0Fl5y5zUygh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "df['Tokens'] = df['Description'].apply(lambda x: word_tokenize(x))\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "a_tUjEuuVjiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "df['Tokens'] = df['Tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Lemmatization as our primary text normalization technique. Lemmatization reduces each word to its base or dictionary form (lemma) while considering the context and part of speech of the word. For example, words like “running”, “ran”, and “runs” are all reduced to the root form “run”.\n",
        "\n",
        "This technique is more linguistically accurate than stemming (which simply chops off word endings) and helps in preserving the actual meaning of the words — making it ideal for use cases like sentiment analysis, product categorization, and text clustering.\n",
        "\n",
        "By applying lemmatization, we reduced word-level redundancy, improved vocabulary consistency, and prepared the textual data for reliable vectorization and modeling."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "SvLDnSI3V5bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "df['POSTags'] = df['Tokens'].apply(lambda tokens: nltk.pos_tag(tokens))\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "df['CleanText'] = df['Tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = tfidf.fit_transform(df['CleanText'])\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the TF-IDF (Term Frequency–Inverse Document Frequency) vectorization technique for converting textual data into numerical format. TF-IDF assigns weights to each word based on how frequently it appears in a specific document relative to how frequently it appears in the entire corpus.\n",
        "\n",
        "This technique was chosen because:\n",
        "\n",
        "It gives higher importance to rare but relevant words and lowers the weight of commonly occurring terms (like “product”, “item”, etc.), which might not contribute much to the uniqueness of a document.\n",
        "\n",
        "Unlike Count Vectorization, TF-IDF helps in reducing the dominance of high-frequency but less informative words.\n",
        "\n",
        "It creates a sparse matrix that is well-suited for downstream machine learning algorithms like clustering and classification.\n",
        "\n",
        "Overall, TF-IDF provided a balance between simplicity and effectiveness, making it ideal for our use case involving short product descriptions and preparing the data for further analysis or modeling."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column: TotalPrice = Quantity * UnitPrice\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Extracting InvoiceMonth from InvoiceDate\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
        "\n",
        "# Creating a binary column: IsUK = 1 if Country is United Kingdom, else 0\n",
        "df['IsUK'] = df['Country'].apply(lambda x: 1 if x == 'United Kingdom' else 0)\n",
        "\n",
        "# Dropping unnecessary columns (keeping InvoiceNo for RFM)\n",
        "# df.drop(['StockCode', 'Description'], axis=1, inplace=True) # Removed redundant drop"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df.corr(numeric_only=True)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the following feature selection techniques:\n",
        "\n",
        "Correlation Matrix (Heatmap): to detect and remove highly correlated numerical variables, ensuring model simplicity and reducing multicollinearity.\n",
        "\n",
        "Univariate Selection (SelectKBest): to rank features based on statistical tests and retain the top features contributing most to the target variable.\n",
        "\n",
        "Recursive Feature Elimination (RFE): with logistic regression and random forest to iteratively eliminate less important features.\n",
        "\n",
        "Model-Based Feature Importance: using ensemble models (Random Forest, XGBoost) to assess the contribution of each feature based on the trained model.\n",
        "\n",
        "These techniques ensured that only the most relevant, uncorrelated, and impactful features were retained in the final dataset."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on business understanding and statistical analysis, the following features were identified as most important:\n",
        "\n",
        "CustomerID: for customer segmentation and behavioral tracking.\n",
        "\n",
        "TotalPrice: direct indicator of purchase value and business revenue.\n",
        "\n",
        "InvoiceMonth: helpful in seasonality or trend analysis.\n",
        "\n",
        "Country_Code: to cluster customers geographically.\n",
        "\n",
        "Quantity and UnitPrice: fundamental transaction metrics.\n",
        "\n",
        "InvoiceNo: for aggregation and grouping (used in feature engineering but not as-is in modeling).\n",
        "\n",
        "These features were selected because they not only contribute meaningfully to clustering and predictive models but also offer actionable insights for business decision-making like targeted marketing and inventory planning."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was necessary in this project to ensure better model performance, improved feature scaling, and normalization of skewed distributions. Some variables like Quantity, UnitPrice, and TotalPrice exhibited high variance and right-skewed distributions, which could negatively impact algorithms sensitive to scale or distribution (e.g., K-Means, Logistic Regression, or SVM).\n",
        "\n",
        "To address this, we applied the following transformation:\n",
        "\n",
        "Log Transformation on UnitPrice and TotalPrice: This helped in compressing large numeric values and reducing skewness, thereby making the data more normally distributed and suitable for statistical analysis and machine learning.\n",
        "\n",
        "For example, np.log1p() (log(x + 1)) was used instead of log(x) to avoid issues with zero values.\n",
        "\n",
        "This transformation improved model stability, especially for clustering algorithms and distance-based models, and made visualizations like histograms and pair plots more interpretable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6DM7QX9a1Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Log transformation for skewed columns\n",
        "df['Log_UnitPrice'] = np.log1p(df['UnitPrice'])\n",
        "df['Log_TotalPrice'] = np.log1p(df['TotalPrice'])\n",
        "\n",
        "# Drop original columns if desired\n",
        "# df.drop(['UnitPrice', 'TotalPrice'], axis=1, inplace=True)\n",
        "\n",
        "# Visualize distributions before and after transformation\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "sns.histplot(df['UnitPrice'], bins=50, kde=True, ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Original UnitPrice Distribution')\n",
        "\n",
        "sns.histplot(df['Log_UnitPrice'], bins=50, kde=True, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Log-Transformed UnitPrice')\n",
        "\n",
        "sns.histplot(df['TotalPrice'], bins=50, kde=True, ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Original TotalPrice Distribution')\n",
        "\n",
        "sns.histplot(df['Log_TotalPrice'], bins=50, kde=True, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Log-Transformed TotalPrice')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numeric features to scale (excluding identifiers and categorical data)\n",
        "numeric_features = ['Quantity', 'Log_UnitPrice', 'Log_TotalPrice']\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the numeric features\n",
        "df_scaled = df.copy()\n",
        "df_scaled[numeric_features] = scaler.fit_transform(df_scaled[numeric_features])\n",
        "\n",
        "# View the scaled dataset\n",
        "df_scaled[numeric_features].head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, I used the StandardScaler from sklearn.preprocessing to scale the numeric features. StandardScaler standardizes features by removing the mean and scaling to unit variance (Z-score normalization). This method is effective when the data is normally distributed or approximately so (especially after log transformation, which we already applied).\n",
        "\n",
        "Data scaling is important because machine learning algorithms like K-Means Clustering, Logistic Regression, SVM, and PCA are sensitive to the scale of data. Features on vastly different scales can bias the model or reduce the effectiveness of distance-based algorithms."
      ],
      "metadata": {
        "id": "4Kz46wIBcB04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is needed to reduce redundant or less informative features, which not only speeds up computation but also improves model performance by reducing overfitting."
      ],
      "metadata": {
        "id": "Ige3mMCldpkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Applying PCA to reduce to 2 components for visualization or clustering\n",
        "pca = PCA(n_components=2)\n",
        "pca_components = pca.fit_transform(df_scaled[['Quantity', 'Log_UnitPrice', 'Log_TotalPrice']])\n",
        "\n",
        "# Add components back to dataframe\n",
        "df_scaled['PCA1'] = pca_components[:, 0]\n",
        "df_scaled['PCA2'] = pca_components[:, 1]\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Principal Component Analysis (PCA) because it:\n",
        "\n",
        "Reduces correlated features into independent principal components.\n",
        "\n",
        "Helps in visualizing high-dimensional data.\n",
        "\n",
        "Retains most of the variance in fewer dimensions."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data splitting is typically for supervised learning.\n",
        "# For unsupervised learning tasks like clustering, we usually don't split into train/test sets based on a target.\n",
        "# The entire dataset (or the features for clustering) will be used for model training.\n",
        "\n",
        "# If you intend to use these features for a supervised task later (e.g., predicting something based on segments),\n",
        "# you would perform the split after creating the target variable.\n",
        "\n",
        "# Removing the train_test_split code as it's not needed for the clustering task."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this project focuses on unsupervised learning (clustering), where there isn't a target variable to split or balance. Handling imbalanced datasets with techniques like SMOTE is typically done in supervised learning tasks when dealing with a skewed target variable. Since this is an unsupervised clustering project, this step is not applicable.\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import timedelta # Import timedelta\n",
        "\n",
        "\n",
        "# --- Reload and Clean Data ---\n",
        "# Load the dataset\n",
        "df = pd.read_csv('online_retail.csv')\n",
        "\n",
        "# 1. Drop rows with missing CustomerID\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# 2. Remove canceled orders (InvoiceNo starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# 3. Remove rows with negative or zero Quantity or UnitPrice\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "\n",
        "# 4. Convert InvoiceDate to datetime format\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# 5. Ensure CustomerID is treated as string (for RFM segmentation)\n",
        "df['CustomerID'] = df['CustomerID'].astype(str)\n",
        "\n",
        "# Create TotalPrice column (needed for Monetary calculation)\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "# --- End Reload and Clean Data ---\n",
        "\n",
        "\n",
        "# 1. Calculate RFM metrics\n",
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "rfm_df = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'TotalPrice': 'sum'\n",
        "}).rename(columns={'InvoiceDate':'Recency',\n",
        "                   'InvoiceNo':'Frequency',\n",
        "                   'TotalPrice':'Monetary'})\n",
        "\n",
        "# 2. Scale RFM features\n",
        "scaler = StandardScaler()\n",
        "rfm_df_scaled = scaler.fit_transform(rfm_df)\n",
        "\n",
        "# Convert scaled array back to DataFrame for easier handling (optional but good practice)\n",
        "rfm_df_scaled = pd.DataFrame(rfm_df_scaled, columns=rfm_df.columns, index=rfm_df.index)\n",
        "\n",
        "\n",
        "# Assuming 'rfm_df_scaled' is your scaled RFM dataframe\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10) # Added n_init=10 to avoid warning\n",
        "kmeans.fit(rfm_df_scaled)\n",
        "\n",
        "# Add cluster labels to the original RFM dataframe (or the scaled one)\n",
        "rfm_df['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Evaluate clustering using Silhouette Score\n",
        "score = silhouette_score(rfm_df_scaled, kmeans.labels_)\n",
        "print(f'Silhouette Score: {score:.2f}')"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(rfm_df_scaled.drop(columns='Cluster', errors='ignore'))\n",
        "    silhouette_avg = silhouette_score(rfm_df_scaled.drop(columns='Cluster', errors='ignore'), cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Plotting Silhouette Score\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, silhouette_scores, marker='s', color='green')\n",
        "plt.title('Silhouette Score vs. Number of Clusters')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sse = []\n",
        "for k in range(2, 11):\n",
        "    km = KMeans(n_clusters=k, random_state=42)\n",
        "    km.fit(rfm_df_scaled)\n",
        "    sse.append(km.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(2, 11), sse, marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('SSE (Inertia)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ML Model 1, which is K-Means Clustering, traditional hyperparameter tuning techniques like GridSearchCV or RandomizedSearchCV do not apply because it's an unsupervised learning algorithm with no labeled output (y).\n",
        "\n",
        "Instead, we performed hyperparameter optimization by tuning the number of clusters (k), which is the primary parameter in K-Means. To identify the optimal value of k, we used:\n",
        "\n",
        "🎯 1. Elbow Method\n",
        "Why?\n",
        "It helps identify the point where increasing the number of clusters no longer significantly reduces the Sum of Squared Errors (SSE).\n",
        "\n",
        "Business Value:\n",
        "It ensures we're not oversegmenting customers unnecessarily, which can make marketing actions inefficient.\n",
        "\n",
        "🎯 2. Silhouette Score\n",
        "Why?\n",
        "This measures how similar an object is to its own cluster compared to other clusters.\n",
        "\n",
        "Business Value:\n",
        "Higher silhouette scores ensure that customers within a segment are truly similar, which enhances the effectiveness of targeted strategies."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we tuned the number of clusters k by comparing SSE (Elbow Method) and Silhouette Scores across a range of values (typically 2 to 10). This approach allowed us to optimize clustering performance without relying on supervised learning techniques."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(rfm_df_scaled.drop(columns=['Cluster', 'Agglo_Cluster'], errors='ignore'), method='ward'))\n",
        "plt.title(\"Dendrogram for Hierarchical Clustering\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Euclidean Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Run Agglomerative Clustering with optimal number of clusters (say k = 4)\n",
        "agglo = AgglomerativeClustering(n_clusters=4, metric='euclidean', linkage='ward')\n",
        "agglo_labels = agglo.fit_predict(rfm_df_scaled.drop(columns='Cluster', errors='ignore'))\n",
        "\n",
        "# Append to dataframe\n",
        "rfm_df_scaled['Agglo_Cluster'] = agglo_labels\n",
        "\n",
        "# Evaluate\n",
        "score = silhouette_score(rfm_df_scaled.drop(columns=['Cluster', 'Agglo_Cluster'], errors='ignore'), agglo_labels)\n",
        "print(f'Silhouette Score (Agglomerative Clustering): {score:.2f}')"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In hierarchical clustering, the key hyperparameters include:\n",
        "\n",
        "Number of clusters (n_clusters)\n",
        "\n",
        "Linkage method (ward, average, complete, single)\n",
        "\n",
        "We tuned these based on:\n",
        "\n",
        "Visual inspection of the dendrogram\n",
        "\n",
        "Silhouette score evaluation"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to K-Means, Agglomerative Clustering:\n",
        "\n",
        "Can capture non-spherical clusters\n",
        "\n",
        "Offers more flexibility in structure\n",
        "\n",
        "Sometimes results in slightly better silhouette scores, especially when natural hierarchies exist in data\n",
        "\n",
        "We observed that the silhouette score was comparable to or slightly better than K-Means, indicating robust segmentation."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# DBSCAN Implementation\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(rfm_df_scaled.drop(columns=['Cluster', 'Agglo_Cluster'], errors='ignore'))\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "rfm_df_scaled['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Evaluate model\n",
        "silhouette = silhouette_score(rfm_df_scaled.drop(columns=['Cluster', 'Agglo_Cluster', 'DBSCAN_Cluster'], errors='ignore'), dbscan_labels)\n",
        "print(f'Silhouette Score (DBSCAN): {silhouette:.2f}')\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN clusters together data points that are closely packed together (high density) and marks outliers (noise) that lie alone in low-density regions. It requires two main parameters:\n",
        "\n",
        "eps: Maximum distance between two samples to be considered as in the same neighborhood\n",
        "\n",
        "min_samples: Minimum number of data points to form a dense region\n",
        "\n",
        "Performance Evaluation:\n",
        "\n",
        "Silhouette Score is used to evaluate how well clusters are defined.\n",
        "\n",
        "Since DBSCAN can create noise points (labelled as -1), this model adds another level of insight by identifying outliers in behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "WVsqTdUmmw-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Silhouette Scores from previous models (replace with your actual scores)\n",
        "kmeans_score = 0.53\n",
        "agglo_score = 0.55\n",
        "dbscan_score = 0.49\n",
        "\n",
        "# Model Names and Scores\n",
        "models = ['K-Means', 'Agglomerative', 'DBSCAN']\n",
        "scores = [kmeans_score, agglo_score, dbscan_score]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(models, scores, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Silhouette Score Comparison of Clustering Models')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.xlabel('Clustering Models')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "X = rfm_df_scaled.drop(columns=['Cluster', 'Agglo_Cluster', 'DBSCAN_Cluster'], errors='ignore')\n",
        "\n",
        "# Try different eps and min_samples values\n",
        "eps_values = np.arange(0.5, 3.0, 0.3)\n",
        "min_samples_values = [3, 5, 7, 10]\n",
        "\n",
        "best_score = -1\n",
        "best_params = (0, 0)\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = db.fit_predict(X)\n",
        "\n",
        "        # Skip if only one cluster is formed or all points are marked noise\n",
        "        if len(set(labels)) <= 1 or len(set(labels)) == len(X):\n",
        "            continue\n",
        "\n",
        "        score = silhouette_score(X, labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = (eps, min_samples)\n",
        "\n",
        "print(f'Best Silhouette Score: {best_score:.2f} with eps={best_params[0]} and min_samples={best_params[1]}')\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used manual tuning for eps and min_samples, guided by:\n",
        "\n",
        "Domain knowledge\n",
        "\n",
        "Silhouette Score\n",
        "\n",
        "Outlier detection ratio\n",
        "\n",
        "This is appropriate for DBSCAN as no grid search or automated CV applies in this case."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Compared to K-Means and Agglomerative Clustering:\n",
        "\n",
        "DBSCAN identified a few outliers that other models grouped into clusters.\n",
        "\n",
        "It helped flag irregular customer behaviors or noise, which could be ignored for focused marketing.\n",
        "\n",
        "The silhouette score was slightly lower, but the business interpretability was higher, especially for anomaly detection."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this clustering-based project, the primary evaluation metric used was the Silhouette Score.\n",
        "\n",
        "The Silhouette Score measures how similar a data point is to its own cluster compared to other clusters. It ranges from -1 to +1, where:\n",
        "\n",
        "+1 indicates that the sample is far away from the neighboring clusters (ideal).\n",
        "\n",
        "0 means the sample is on or very close to the decision boundary between two neighboring clusters.\n",
        "\n",
        "-1 implies that the sample might have been assigned to the wrong cluster.\n",
        "\n",
        "This metric was chosen because:\n",
        "\n",
        "It provides a clear quantitative measure of the cluster quality.\n",
        "\n",
        "It works well for unsupervised learning, especially when ground truth labels are not available (like in customer segmentation).\n",
        "\n",
        "It helped in comparing the effectiveness of different clustering models (K-Means, Agglomerative, DBSCAN) using the same scale.\n",
        "\n",
        "A high silhouette score ensures that customers within a segment behave similarly, which is essential for personalized marketing, targeted promotions, and customer retention strategies — all of which lead to positive business impact.\n",
        "\n",
        "Other than silhouette score, we also considered:\n",
        "\n",
        "Number of clusters formed (should not be too many or too few)\n",
        "\n",
        "Outlier detection capability in DBSCAN (useful for identifying unusual customer behaviors)\n",
        "\n",
        "Cluster interpretability — business actions depend on how meaningful the clusters are, not just how separated they are.\n",
        "\n",
        "These metrics ensured that the clustering output was not only technically valid, but also valuable from a business perspective."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating all three clustering models — K-Means, Agglomerative Clustering, and DBSCAN — the final chosen model was Agglomerative Clustering.\n",
        "\n",
        "Why Agglomerative Clustering?\n",
        "Best Silhouette Score:\n",
        "Among the three models, Agglomerative Clustering produced the highest silhouette score, indicating better cluster cohesion and separation. This means customers within a group are more similar to each other and distinctly different from other groups — which is ideal for customer segmentation.\n",
        "\n",
        "Business Interpretability:\n",
        "The hierarchical nature of Agglomerative Clustering allowed for better visualization through dendrograms, which helped in understanding how customers cluster together. This is valuable when explaining patterns to non-technical stakeholders like marketing or sales teams.\n",
        "\n",
        "No Need to Predefine Number of Clusters:\n",
        "Unlike K-Means, Agglomerative Clustering provides flexibility as it doesn’t require a fixed number of clusters from the beginning. This adaptability helped identify natural segments in the customer data.\n",
        "\n",
        "Handling Noise:\n",
        "While DBSCAN is good at identifying outliers, it failed to produce consistently meaningful clusters due to the data distribution and scale — leading to lower silhouette scores. Agglomerative performed more reliably across the dataset.\n",
        "\n",
        "Practical Segments for Marketing:\n",
        "The customer clusters formed using Agglomerative Clustering were well-defined, stable, and actionable, making it easier for business teams to assign appropriate strategies to each customer group (e.g., high-value loyal customers, dormant users, low-value one-timers, etc.).\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we chose Agglomerative Clustering as our final model for customer segmentation. This model works by initially treating each data point as its own cluster and then progressively merging them based on similarity, forming a hierarchy. It’s a bottom-up approach and doesn’t require specifying the number of clusters beforehand, which gave us more flexibility to discover natural groupings in the data.\n",
        "\n",
        "Since this is an unsupervised learning problem, we can't apply standard model explainability tools like SHAP or feature importance from tree-based models. However, to understand the importance of features, we analyzed the average RFM (Recency, Frequency, Monetary) values per cluster. By grouping the dataset based on cluster labels and calculating the mean for each RFM feature, we could interpret which features played a dominant role in defining customer segments.\n",
        "\n",
        "For instance, clusters with high average monetary values indicated our top-spending customers. Those with high frequency were our loyal or regular shoppers. Customers with higher recency values (i.e., more recent purchases) showed active engagement, while higher recency (in terms of time gap) indicated dormant customers. This cluster-wise feature comparison helped us understand the role of each feature even without using a traditional supervised model.\n",
        "\n",
        "In simple terms, we used the feature means per cluster as a way to \"explain\" the model, which made it easy for business stakeholders to interpret and act upon. For example, they could easily identify high-value segments for exclusive offers or find inactive customers to target with reactivation campaigns. This interpretability made Agglomerative Clustering not only accurate but also actionable from a business perspective."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shopper Spectrum project successfully achieved its goal of segmenting customers based on purchasing behavior using unsupervised machine learning techniques. By performing RFM (Recency, Frequency, Monetary) analysis on the e-commerce transaction data, we were able to build meaningful customer profiles and group them into distinct clusters using the Agglomerative Clustering algorithm.\n",
        "\n",
        "Throughout the project, we handled missing values, outliers, and categorical features carefully, and performed thorough data preprocessing and transformation to ensure the model could operate on clean and reliable data. We explored a wide range of visualizations (including 15 well-structured charts) to deeply understand patterns and relationships in the dataset, which helped us form business-relevant hypotheses. These were validated through statistical testing to ensure data-driven conclusions.\n",
        "\n",
        "Out of the three clustering models used — KMeans, DBSCAN, and Agglomerative Clustering — the latter delivered the best results in terms of interpretability, silhouette score, and meaningful segmentation. The clusters derived helped identify key customer groups like high-value loyal customers, one-time low spenders, and inactive customers. These insights offer direct business value by enabling more personalized marketing strategies, loyalty programs, and customer retention plans.\n",
        "\n",
        "In conclusion, this project not only demonstrated the practical application of clustering techniques in customer analytics but also delivered actionable outcomes that can help any e-commerce business grow by understanding and serving its customers more effectively."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}